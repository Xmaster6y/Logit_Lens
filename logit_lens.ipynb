{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJGM6TFrFtU-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u33PYN2NHN5s"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfQIeRiUC5l3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import GPT2Config, GPT2Model\n",
        "from transformers.models.gpt2 import modeling_gpt2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9WhcmQoFr-c"
      },
      "source": [
        "## Define GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb2Py6rrFp5n"
      },
      "outputs": [],
      "source": [
        "# Initializing a model (with random weights) from the configuration\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCyD2yILhhg"
      },
      "outputs": [],
      "source": [
        "block_list = []\n",
        "for module in model.modules():\n",
        "    print('------------------')\n",
        "    print(f'{type(module)}:\\n{module}')\n",
        "    print('------------------')\n",
        "    if isinstance(module, modeling_gpt2.GPT2Block):\n",
        "        block_list.append(module)\n",
        "print(f'Number of blocks: {len(block_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e97InZDiFp-J"
      },
      "outputs": [],
      "source": [
        "print(help(model.register_forward_hook))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O45Xljo1Fq1H"
      },
      "source": [
        "## Define Hooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upNoA0YVRVlm"
      },
      "outputs": [],
      "source": [
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.module_out = None\n",
        "        self.module_in = None\n",
        "\n",
        "    def __call__(self, module, module_in, module_out):\n",
        "        \"\"\"Forward pytorch hook\"\"\"\n",
        "        self.module_out = module_out\n",
        "        self.module_in = module_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9zjM0YhFqEo"
      },
      "outputs": [],
      "source": [
        "class BlockHook:\n",
        "    def __init__(self, model:modeling_gpt2.GPT2Model, block_num:int, debug:bool=False):\n",
        "        self.block = [module for module in model.modules() if isinstance(module, modeling_gpt2.GPT2Block)][block_num]\n",
        "        self.hook = SaveOutput()\n",
        "        self.debug = debug\n",
        "        self.block.register_forward_hook(self.hook)\n",
        "        self.wte, _ = [module for module in model.modules() if isinstance(module, torch.nn.modules.sparse.Embedding)]\n",
        "        self.ln_f = [module for module in model.modules() if isinstance(module, torch.nn.modules.normalization.LayerNorm)][-1]\n",
        "\n",
        "    def get_logits(self):\n",
        "        h = self.hook.module_out[0]\n",
        "        print(h.shape)\n",
        "        h = self.ln_f(h)\n",
        "        batch, seq_len, param = h.shape\n",
        "        h_flat = torch.reshape(h, (batch*seq_len, param))\n",
        "        emb_weight = self.wte.weight\n",
        "        if self.debug:\n",
        "            print(type(emb_weight))\n",
        "            print(emb_weight.shape)\n",
        "            print(h.shape)\n",
        "        logits = torch.einsum(\"bcl,tl->bct\", h, emb_weight)\n",
        "        return logits\n",
        "\n",
        "    def get_best_interests(self, num:int=15):\n",
        "        logits = self.get_logits()\n",
        "        prob = logits[0, -1, :].detach().numpy()\n",
        "        prob_ind = zip(prob, np.arange(len(prob)))\n",
        "        my_key = lambda x: x[0]\n",
        "        prob_ind_sorted = sorted(prob_ind, key=my_key, reverse=True)\n",
        "        interests = prob_ind_sorted[:num]\n",
        "        return interests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iMdJTpDFuWc"
      },
      "source": [
        "## Register hook\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "301LOm5rFqHQ"
      },
      "outputs": [],
      "source": [
        "hook_list = [BlockHook(model, i) for i in range(12)]\n",
        "my_hook = hook_list[0]\n",
        "print(my_hook.hook.module_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TytErmV7WEHM"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "token = tokenizer(\"I love eating\", return_tensors=\"pt\")\n",
        "res = model(**token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8b7-moExjFD"
      },
      "outputs": [],
      "source": [
        "for hook in hook_list:\n",
        "    interests = hook.get_best_interests()\n",
        "    print('------------------')\n",
        "    for logit, interest in interests:\n",
        "        print(f\"{logit:.5f} :: {tokenizer.decode(interest)}\")\n",
        "    print('------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe2cjMPBFvEa"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMmPlaqXFvMH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVbKJsyvFvQg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
